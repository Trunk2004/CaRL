hydra:
  run:
    dir: ${output_dir}
  output_subdir: ${output_dir}/code/hydra           # Store hydra's config breakdown here for debugging
  searchpath:                                       # Only <exp_dir> in these paths are discoverable
    - pkg://nuplan.script.config.common
    - pkg://nuplan.script.experiments      # Put experiments configs in script/experiments/<exp_dir>

defaults:
  - default_experiment
  - default_common

  - splitter: nuplan

  - observation_builder: default_observation_builder
  - reward_builder: default_reward_builder
  - scenario_sampler: cache_scenario_sampler
  - simulation_builder: default_simulation_builder
  - trajectory_builder: action_trajectory_builder

debug: False

schedule_free: False
learning_rate: 2.5e-4  # Learning rate of the optimizer
adam_betas: [0.9, 0.999]  # Betas of Adam/AdamW
adam_eps: 1.0e-5  # Eps parameter of adam optimizer

total_timesteps: 100000000  # Total timesteps of the experiments
torch_deterministic: True  # if toggled, `torch.backends.cudnn.deterministic=False`
cuda: True  # Whether cuda will be enabled by default

track: False  # Whether to track experiment with weights and biases (wandb)
wandb_project_name: "ppo_nuplan"  # name of the weights and biases (wandb) project
wandb_entity: null  # Name of the weights and biases (wandb) project
capture_video: False  # Whether to capture videos of the agent performances (check out `videos` folder)


total_batch_size: 4096  # Total amount of data collected at every step across all environments
total_minibatch_size: 1024 # Total minibatch sized used for training (across all environments)

lr_schedule: "linear" #  # Which lr schedule to use. Options: (linear, kl, none, step, cosine, cosine_restart)

gae: True # Use GAE for advantage computation
gamma: 0.99 # The discount factor gamma
gae_lambda: 0.95  # The lambda for the general advantage estimation

update_epochs: 2  # The K epochs to update the policy
norm_adv: True  # Toggles advantages normalization

clip_coef: 0.1  # the surrogate clipping coefficient
clip_vloss: True  # Toggles whether to use a clipped loss for the value function, as per the paper.
ent_coef: 0.0  # Coefficient of the entropy
vf_coef: 0.5 # coefficient of the value function
max_grad_norm: 0.5  # the maximum norm for the gradient clipping
target_kl: 0.015 # the target KL divergence threshold
visualize: False


load_file: null  # Model weights for initialization


train_gpu_ids: null # List of GPU ids to use for training. If None, all GPUs are used.
num_envs_per_gpu: 32 # Number of environments to put on one GPU.

expl_coef: 0.05  # Coefficient of the exploration
cpu_collect: False  #  Whether the agent will be run on the cpu during data collection (can be faster)

use_exploration_suggest: False #  Whether to use the exploration loss from roach.
beta_min_a_b_value: 1.0  # Nugget that gets added to the softplus output of the network.

use_rpo: False # Whether to use robust policy optimization (add noise to mean during training)
rpo_alpha: 0.5  # Noise added during training is of uniform shape [-rpo_alpha, rpo_alpha]
# obs_num_channels: 9  # Number of channels is the BEV image.
pixels_per_meter2: 5.0 # Pixels per meter in the new BEV image.
eval_time: 1200 # Time until the model times out.

terminal_reward: 0.0  # Reward at the end of the episode

normalize_rewards: False  # Whether to use gymnasiums reward normalization.
image_encoder: "roach_ln2"
use_layer_norm: True # Whether to use LayerNorm before ReLU in MLPs.
use_world_model_loss: False  # Whether to let the model predict the next frame as auxiliary task during the training
world_model_loss_weight: 1.0 # Weight of the world model loss.

distribution: "beta"  # Distribution used for the action space. Options beta, normal, beta_uni_mix
weight_decay: 0.0 # Weight decay applied to optimizer. AdamW is used when > 0.0
use_dd_ppo_preempt: False  # Whether to use the dd-ppo preemption technique to early stop stragglers
use_termination_hint: False

use_perc_progress: False  # Whether to multiply RC reward by percentage away from lane center.
use_extra_control_inputs: False  # Whether to use extra control inputs such as integral of past steering.

use_layer_norm_policy_head: True  # Applicable if use_layer_norm=True, whether to also apply layernorm to the policy head.
use_hl_gauss_value_loss: False  # Whether to use the histogram loss gauss to train the value head via classification
use_outside_route_lanes: False # Whether to terminate the route when invading opposing lanes or sidewalks.
use_max_change_penalty: False

# Cache parameters
cache:
  cache_path: ${oc.env:NUPLAN_EXP_ROOT}/caches/gym_cache         # Local path to store scenario cache
  format: "gz" # [gz, msgpack, json]                            # Format to save the cache in
  compression_level: 1

# Mandatory parameters
py_func: ???                                          # Function to be run inside main (can be "train", "test", "cache")
